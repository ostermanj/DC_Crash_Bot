{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis - DC Moving Violations, 2015-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook looks at fine payments, moving violation counts and moving violation types across wards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc-crash-bot-test.cw2qdhdq18cy.us-east-1.rds.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "##Import Libraries\n",
    "import psycopg2 #PostgreSQL driver\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pandas.io.sql as psql\n",
    "import geopandas as gpd\n",
    "import SQL_cred\n",
    "from collections import Counter\n",
    "print(SQL_cred.PGHOST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Ward Population Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>ward_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91673</td>\n",
       "      <td>Ward 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92809</td>\n",
       "      <td>Ward 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84979</td>\n",
       "      <td>Ward 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87150</td>\n",
       "      <td>Ward 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90380</td>\n",
       "      <td>Ward 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>103197</td>\n",
       "      <td>Ward 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>80669</td>\n",
       "      <td>Ward 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80517</td>\n",
       "      <td>Ward 8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Population ward_name\n",
       "0       91673    Ward 1\n",
       "1       92809    Ward 2\n",
       "2       84979    Ward 3\n",
       "3       87150    Ward 4\n",
       "4       90380    Ward 5\n",
       "5      103197    Ward 6\n",
       "6       80669    Ward 7\n",
       "7       80517    Ward 8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Info Pulled From https://www.dchealthmatters.org/demographicdata\n",
    "dc_ward_pop=pd.DataFrame({'Population':[91673,92809,84979,87150,90380,103197,80669,80517],'ward_name':['Ward 1','Ward 2','Ward 3','Ward 4','Ward 5','Ward 6','Ward 7','Ward 8']})\n",
    "dc_ward_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Payment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data, Basic Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set SQL Address, Create SQL import function 'load_data'\n",
    "conn_string = \"host=\"+ SQL_cred.PGHOST +\" port=\"+ \"5432\" +\" dbname=\"+ SQL_cred.PGDATABASE +\" user=\" + SQL_cred.PGUSER \\\n",
    "+\" password=\"+ SQL_cred.PGPASSWORD\n",
    "\n",
    "def load_data(query):\n",
    "\n",
    "    sql_command = query\n",
    "    print (sql_command)\n",
    "\n",
    "    # Load the data\n",
    "    data = pd.read_sql(sql_command, conn)\n",
    "\n",
    "    print(data.shape)\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select all fine information from 2015 to 2020\n",
    "conn=psycopg2.connect(conn_string)\n",
    "query=\"SELECT fine_amount,total_paid, ward_name FROM analysis_data.moving_violations WHERE EXTRACT (year FROM issue_date) BETWEEN 2015 AND 2020;\"\n",
    "cursor = conn.cursor()\n",
    "df=load_data(query)\n",
    "df.to_csv('Fines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Fines.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bb/yyg2_7f16cs_lqhm99fbs0t00000gn/T/ipykernel_24728/4116230234.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fines.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Fines.csv'"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('Fines.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fine_amount.fillna(0,inplace=True)\n",
    "df.total_paid.fillna(0,inplace=True)\n",
    "df.ward_name.fillna('Unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Categories for fines & how they were paid\n",
    "FvP=[]\n",
    "for fine,paid in list(zip(df.fine_amount,df.total_paid)):\n",
    "    if fine==0 and paid==0:\n",
    "        FvP.append('No Fine')\n",
    "    elif fine==paid:\n",
    "        FvP.append('Fined & Paid Full')\n",
    "    elif fine>0 and paid==0:\n",
    "        FvP.append('Fined & Paid Nothing')\n",
    "    elif fine>0 and fine>paid:\n",
    "        FvP.append('Fined & Paid Less')\n",
    "    elif fine>0 and fine<paid:\n",
    "        FvP.append('Fined & Paid More')\n",
    "    else:\n",
    "        FvP.append('Na')\n",
    "df['Fined_v_Paid']=FvP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How fines were handled for violations in all of DC, 2015-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fines_alldc=pd.concat([df['Fined_v_Paid'].value_counts(),(df['Fined_v_Paid'].value_counts()/df['Fined_v_Paid'].value_counts().sum())*100],axis=1)\n",
    "df_fines_alldc.columns=['Count','Percentage']\n",
    "df_fines_alldc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group fine counts by ward and fine type\n",
    "df_grouped_fine=df.groupby(['ward_name','Fined_v_Paid']).count().reset_index(level=[0,1])\n",
    "df_fine_count=df_grouped_fine.pivot(index='ward_name',columns=['Fined_v_Paid'],values='fine_amount').drop(columns='Na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a data frame with fine percentages by ward\n",
    "df_fine_pct=df_fine_count.apply((lambda x: x/x.sum()),axis=1)\n",
    "df_fine_count['Total']=df_fine_count.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of how fines were handled for traffic incidents by ward, sorted by total, 2015-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fine_count.sort_values('Total',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentages of how fines were handled for traffic incidents by ward, sorted by Paid Nothing, 2015-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_fine_pct=df_fine_pct*100\n",
    "df_fine_pct.sort_values('Fined & Paid Nothing', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Violation Count & Types by Ward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violation Counts by Ward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query database, save csv to local files\n",
    "#Select all violation descriptons from 2015 to 2020 along with road type & ward name\n",
    "#conn=psycopg2.connect(conn_string)\n",
    "#query=\"SELECT ward_name, violation_process_desc, dcfunctionalclass_desc FROM analysis_data.moving_violations WHERE EXTRACT (year FROM issue_date) BETWEEN 2015 AND 2020;\"\n",
    "#cursor = conn.cursor()\n",
    "#df_violations=load_data(query)\n",
    "#df_violations.rename(columns={\"violation_process_desc\":\"Violations\"},inplace=True)\n",
    "#df_violations.to_csv('Violations_by_Ward_2015_2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in local csv data\n",
    "df_violations=pd.read_csv('Violations_by_Ward_2015_2020.csv',usecols=[1,2,3])\n",
    "df_violations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://opendata.dc.gov/datasets/0ef47379cbae44e88267c01eaec2ff6e_31?geometry=-77.668%2C38.800%2C-76.361%2C38.987\n",
    "#https://towardsdatascience.com/lets-make-a-map-using-geopandas-pandas-and-matplotlib-to-make-a-chloropleth-map-dddc31c1983d\n",
    "b='DC_Ward_Map/Ward_from_2012.shp'\n",
    "map_df=gpd.read_file(b)\n",
    "map_df=map_df.sort_values('WARD_ID')\n",
    "map_df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get total violation counts per ward, per capita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViolationsByward=df_violations.groupby('ward_name').count()\n",
    "ViolationsByward.reset_index(inplace=True)\n",
    "ViolationsByward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViolationsBywardPop=ViolationsByward.merge(dc_ward_pop)\n",
    "ViolationsBywardPop['TotalViolationsPerCap']=ViolationsBywardPop.Violations/ViolationsBywardPop.Population\n",
    "ViolationsBywardPop=ViolationsBywardPop.sort_values('TotalViolationsPerCap',ascending=False).drop(columns='dcfunctionalclass_desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViolationsBywardPop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df=map_df.merge(ViolationsBywardPop[['ward_name','TotalViolationsPerCap']],left_on='NAME',right_on='ward_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = \"TotalViolationsPerCap\"\n",
    "vmin, vmax = 120, 220\n",
    "fig, ax = plt.subplots(1, figsize=(16, 8))\n",
    "map_df.plot(column=variable, cmap=\"Blues\", linewidth=0.8, ax=ax, edgecolor='0.8',legend=True)\n",
    "plt.title('Total Moving Violations per Capita 2015-2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViolationsBywardPop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ward 5, 7 & 8 have the highest per capita violations - Ward 7 & 8 have at least 2.5x greater than Wards 4, 3, 6 & 1. \n",
    "\n",
    "\n",
    "##### But wards 5, 7 & 8 also have the most highways - presumably . Can the data be normalized by which types of roads they occur on?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the road type where violations occur most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill unknown road types with 'Unknown'\n",
    "df_violations['dcfunctionalclass_desc']=df_violations['dcfunctionalclass_desc'].fillna('Unknown')\n",
    "df_violations.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by ward & road type\n",
    "ViolationsByRoadtype=df_violations.groupby(['ward_name','dcfunctionalclass_desc']).count()\n",
    "ViolationsByRoadtype.reset_index(inplace=True)\n",
    "ViolationsByRoadtype.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Looks like a lot of road types are unknown for these violations. Is this systematic across wards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at just the unknown road types\n",
    "ViolationsUnknownLocationType=ViolationsByRoadtype.loc[ViolationsByRoadtype['dcfunctionalclass_desc']=='Unknown']\n",
    "ViolationsUnknownLocationType=ViolationsUnknownLocationType.merge(ViolationsBywardPop,on='ward_name',suffixes=['_Unkn','_Tot'])\n",
    "ViolationsUnknownLocationType\n",
    "#ViolationsByward2_['NormalizedViolations']=ViolationsByward2.Violations/ViolationsByward2.Population\n",
    "#ViolationsByward2_.sort_values('NormalizedViolations',ascending=False).drop(columns='dcfunctionalclass_desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViolationsUnknownLocationType['Unknown_by_ward']=ViolationsUnknownLocationType.Violations_Unkn/ViolationsUnknownLocationType.Violations_Tot\n",
    "ViolationsUnknownLocationType.sort_values('Unknown_by_ward',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unfortunately, the number of Unknown road types is extremely spread out across wards - accounting for 95% of incidents for ward 3 and just 6 % for ward 4, with all the other roads scattered in between. We'll need to fill in more of these data points to normalize by this value.\n",
    "##### Instead, the types of violations across wards can be investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violation Types by Ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print all unique violation types\n",
    "pd.set_option('display.max_rows', None)\n",
    "df_violations.Violations.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separately, I downloaded these unique values into a csv file called 'Violation_Categories.csv', and assigned them one of the following categories:\n",
    "##### D: Immediately Dangerous to Others \n",
    "Dangerous driving behavior that is obviously in violation of traffic laws e.g. speeding, improper turning\n",
    "##### S: Secondary Conduct Violations  \n",
    "Violations that are not obvious from driver behavior (e.g. no insurance) or pertain to the drivers vehicle (e.g. broken headlights)\n",
    "##### MISC: Miscellaneous  \n",
    "Commercial, truck, taxi violations, etc\n",
    "\n",
    "#### These categories and their criteria stand to be refined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in categories csv\n",
    "df_categories=pd.read_csv('Categories_csv.csv',index_col=1)\n",
    "dict_cat=df_categories.to_dict()['Category']\n",
    "#Map Categories\n",
    "df_violations['Category']=df_violations.Violations.map(dict_cat)\n",
    "dict2={'S':'Secondary Conduct Violations','D':'Immediately Dangerous to Others','MISC':'Miscellaneous'}\n",
    "df_violations['Category']=df_violations['Category'].map(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in categories dict\n",
    "#with open(\"Categories.json\", \"r\") as infile:  \n",
    "#    dict_=json.load(infile)\n",
    "#df_violations2.rename(mapper={'dcfunctionalclass_desc':'Count'},inplace=True)\n",
    "#df_violations['Category']=df_violations.Violations.map(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new df 'df_violations_' that groups by ward & type of violation\n",
    "df_violations_=df_violations.drop(columns=['dcfunctionalclass_desc']).copy()\n",
    "df_violations_=df_violations_.groupby(['ward_name','Category']).count()\n",
    "df_violations_.reset_index(level=[0,1],inplace=True)\n",
    "df_violations_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge with population data, determine violation types per capita\n",
    "ViolationsByward_=df_violations_.merge(dc_ward_pop,on='ward_name')\n",
    "ViolationsByward_['ViolationsPerCapita']=ViolationsByward_.Violations/ViolationsByward_.Population\n",
    "ViolationsByward_.sort_values('ViolationsPerCapita',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivot data frame: ward is index, columns are type of violation, values are violation per capita\n",
    "ViolationsByward_pivot=ViolationsByward_.pivot(index='ward_name',columns=['Category'],values='ViolationsPerCapita')\n",
    "ViolationsByward_pivot.reset_index(inplace=True)\n",
    "ViolationsByward_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Map dataframe with Violation Type per Capita Data\n",
    "map_df=map_df.merge(ViolationsByward_pivot,left_on='NAME',right_on='ward_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map per capita incidence of \"Immediately Dangerous to Others\" violations\n",
    "variable = \"Immediately Dangerous to Others\"\n",
    "vmin, vmax = 120, 220\n",
    "fig, ax = plt.subplots(1, figsize=(16, 8))\n",
    "map_df.plot(column=variable, cmap=\"Reds\", linewidth=0.8, ax=ax, edgecolor='0.8',legend=True)\n",
    "plt.title('Immediately Dangerous to Others Traffic Violations Per Capita by Ward 2015-2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map per capita incidence of \"\"Secondary Conduct Violations\" violations\n",
    "variable = \"Secondary Conduct Violations\"\n",
    "vmin, vmax = 120, 220\n",
    "fig, ax = plt.subplots(1, figsize=(16, 8))\n",
    "map_df.plot(column=variable, cmap=\"Greens\", linewidth=0.8, ax=ax, edgecolor='0.8',legend=True)\n",
    "plt.title('Secondary Conduct Violations Per Capita by Ward 2015-2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map per capita incidence of \"Miscellaneous\" violations\n",
    "variable = \"Miscellaneous\"\n",
    "vmin, vmax = 120, 220\n",
    "fig, ax = plt.subplots(1, figsize=(16, 8))\n",
    "map_df.plot(column=variable, cmap=\"Purples\", linewidth=0.8, ax=ax, edgecolor='0.8',legend=True)\n",
    "plt.title('Miscellaneous 2015-2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorizer Code - Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Speeding=['MPH','SPEEDING','SPEED']\n",
    "Traffic=['STOP','DISTRACTED','ATTENTION','TURN','LEFT','RIGHT','LANES','STREET','YIELD','TRAFFIC','PASS','DRIVE','DRIVING','INTERSECTION','PEDESTRIAN','CYCLIST','BICYCLE']\n",
    "Permit=['REGISTRATION','REGISTERED','UNREGISTERED','INSURANCE','PERMIT','TAGS']\n",
    "Car=['WINDOW','WINDOWS','LIGHT','LIGHTS','PLATE','BUMPER','BUMPERS','PLATES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=df_violations.Violations\n",
    "p=[i.lower() for i in k[0:600000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Violations=pd.DataFrame(df_violations.Violations.unique(),columns=['Violations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Violations.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Preprocessing\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#Make a dataframe of unique column names\n",
    "Violations=pd.DataFrame(df_violations.Violations.unique(),columns=['Violations'])\n",
    "#Remove 'None' Entry\n",
    "Violations.dropna(axis=0,inplace=True)\n",
    "#Replace any non letters with a white space (the carrot makes it 'replace everything except')\n",
    "Violations['Violations_Edit']= Violations['Violations'].str.replace('[^a-zA-Z]', ' ')\n",
    "#Make all text lower case\n",
    "Violations['Violations_Edit'] = Violations['Violations_Edit'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct Misspellings\n",
    "Violations['Violations_Edit'].loc[1]='owner operate or permit operation of uninsured vehicle'\n",
    "Violations['Violations_Edit'].loc[176]='less than yrs old mandatory seat belt violation'\n",
    "Violations['Violations_Edit'].loc[214]='less than yrs old passenger restraint violation'\n",
    "Violations['Violations_Edit'].loc[114]='bike pmd fail to yield right of way to pedestrian or vehicle'\n",
    "Violations['Violations_Edit'].loc[254]='failure yield row transit bus'\n",
    "\n",
    "#Lemmatize Entires, Remove Stop Words\n",
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_remove_stopwords(text):\n",
    "    y=[lemmatizer.lemmatize(w) for w in word_tokenize(text)]\n",
    "    h=[c for c in y if c not in stopwords.words('english')]\n",
    "    return \" \".join(h)\n",
    "\n",
    "Violations['Violations_Edit']=Violations['Violations_Edit'].apply(lemmatize_remove_stopwords)\n",
    "Violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Violations.to_csv('Violations_Unique.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=Violations['Violations_Edit'].tolist()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(h)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df_vw=pd.DataFrame(X_train_counts.toarray(),columns=count_vect.get_feature_names())\n",
    "df_vw.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=5)\n",
    "model.fit(X_train_counts)\n",
    "components_df = pd.DataFrame(model.components_, columns=df_vw.columns)\n",
    "for i in range(0,5):\n",
    "    component = components_df.iloc[i]\n",
    "    print('Component ',i)\n",
    "    print(component.nlargest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data=model.transform(X_train_counts)\n",
    "df_=pd.DataFrame(transformed_data,index=Violations.Violations)\n",
    "#df_.columns=['Zero','One','Two','Three','Four','Five','Six','Seven','Eight','Nine']\n",
    "df_.columns=['Zero','One','Two','Three','Four']\n",
    "df_['MajorComponent']=df_.idxmax(axis=1).values\n",
    "df_.sort_values('MajorComponent')\n",
    "category_map={'Zero':'Dangerous to Others','One':'Conduct Violations','Two':'Dangerous to Others','Three':'Dangerous to Others','Four':'Conduct Violations',}\n",
    "df_['New_Map']=df_['MajorComponent'].map(category_map)\n",
    "df_.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "norm_features = normalize(transformed_data)\n",
    "df = pd.DataFrame(norm_features,index=Violations.Violations)\n",
    "article = df.iloc[81]\n",
    "# Compute the dot products: similarities\n",
    "similarities = df.dot(article)\n",
    "print(similarities.nlargest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for generating categories via input\n",
    "#dict_={}\n",
    "#for i in list(df_['Violations']):\n",
    "#    txt=input(i+' ')\n",
    "#    dict_[i]=txt\n",
    "#dict_['OPERATE A VEHICLE IN VIOLATION OF A RESTRICTION']=C\n",
    "#dict_['SPEEDING IN CMV UP TO 10 MPH OVER SPEED LIMIT C']=D\n",
    "#dict_['FAIL TO TURN WHEEL TO CURB']=D\n",
    "#import json\n",
    "#with open(\"Categories.json\", \"w\") as outfile:  \n",
    "#    json.dump(dict_, outfile) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
